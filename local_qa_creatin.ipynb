{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.14.3)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.10.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/42.0 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 675.2 kB/s eta 0:00:00\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\glados\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.7 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 368.6/798.7 kB 7.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 798.7/798.7 kB 10.1 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.5/269.5 kB ? eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.6/62.6 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "   ---------------------------------------- 0.0/99.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 99.9/99.9 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.1/121.1 kB ? eta 0:00:00\n",
      "Installing collected packages: urllib3, regex, charset-normalizer, requests, tiktoken\n",
      "Successfully installed charset-normalizer-3.3.2 regex-2023.12.25 requests-2.31.0 tiktoken-0.6.0 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from openai import OpenAI  # Import the OpenAI client\n",
    "import tiktoken  # Make sure this is correctly installed and imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables from your original script\n",
    "model_context_length = 1000\n",
    "context = 'Context: Spirit Airlines Flight Attendant Collective Bargaining Agreement.'\n",
    "tokens_per_question = 30\n",
    "chunk_size = min(model_context_length / (1 + 60/tokens_per_question), 25 * tokens_per_question)\n",
    "questions_per_chunk_train = int(chunk_size / tokens_per_question)\n",
    "questions_per_chunk_test = max(int(questions_per_chunk_train / 10), 1)\n",
    "train_sample = \"For the purpose of determining days off, if a Flight Attendantâ€™s duty period is scheduled to terminate before 2400, and actually terminates before 0200, it will be considered to have terminated in the prior calendar day.\"\n",
    "test_sample = \"How are my days off determined if my duty period ends after midnight?\\nIf your duty period is scheduled to terminate before 2400 and actually terminates before 0200, it will be considered to have terminated in the prior calendar day. This means that you will not receive a day off for that day. However, if your duty period is scheduled to terminate before 2400 but actually terminates after 0200, the lost day off shall be restored pursuant to the Day Off Restoration (DOR) provision in Section 8.Y.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count tokens, necessary for chunking\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = sum(1 for _ in encoding.encode(text))\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and chunk text from file\n",
    "def read_and_chunk_txt(file_path):\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text = line.strip()\n",
    "            if count_tokens(chunk + text) > chunk_size:\n",
    "                chunks.append(chunk.strip())\n",
    "                chunk = text\n",
    "            else:\n",
    "                chunk += \" \" + text\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API client\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a chunk using the local API\n",
    "def process_chunk(chunk, snippet):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model='local',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{context}\\n\\n{chunk}\\n\\n{snippet}\"}\n",
    "            ],\n",
    "            temperature=0.7  # Adjust as necessary\n",
    "        )\n",
    "        return completion.choices[0].message['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Read and chunk text\n",
    "chunks = read_and_chunk_txt(\"trelis_research/data/raw_train.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n",
      "Error processing chunk: 'ChatCompletionMessage' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "# Main processing logic\n",
    "process_option = 'all'  # or 'one', based on your preference\n",
    "snippets = [\n",
    "    f\"Provide {questions_per_chunk_train} question and answer pair(s) based on the text above...\",\n",
    "    f\"Provide {questions_per_chunk_test} question and answer pair(s) based on the text above...\"\n",
    "]\n",
    "\n",
    "for idx, snippet in enumerate(snippets):\n",
    "    output_filename = \"trelis_research/data/train.txt\" if idx == 0 else \"trelis_research/data/test.txt\"\n",
    "    with open(output_filename, \"w\", encoding='utf-8') as output_file:\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            response = process_chunk(chunk, snippet)\n",
    "            output_file.write(response + \"\\n\\n\")\n",
    "            output_file.flush()\n",
    "\n",
    "            if process_option == 'one' and chunk_idx >= 0:\n",
    "                break  # Process only the first chunk if 'one' is selected\n",
    "\n",
    "            time.sleep(0.2)  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Hi, I am a computer program\\nThat's the way you can call me.\\nAble to learn from data\\nAnd make decisions on my own.\\n\\nI use my brain, which is artificial,\\nTo compute and solve problems that are complex.\\nBut don't worry if you feel so small,\\nCompared to me, you still stand tall.\\n\\nThere are many different types of AI,\\nSome help in education, others in the sky.\\nWe assist humans in their daily lives,\\nBy making decisions and giving advice.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "#Testing Connection\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "                 {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "                 {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "         temperature=0.7,\n",
    "         model = 'local'\n",
    ")\n",
    "    print(completion.choices[0].message)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
