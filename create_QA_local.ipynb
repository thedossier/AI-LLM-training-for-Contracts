{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import argparse\n",
    "\n",
    "# # Backup the real `sys.argv`\n",
    "# original_argv = sys.argv.copy()\n",
    "\n",
    "# # Set the sys.argv to mimic command line input\n",
    "# sys.argv = ['script_name', '--context', 'Example context', '--train_output_filename', 'train_output.txt', \n",
    "#             '--test_output_filename', 'test_output.txt', '--api', 'openai', '--temperature', '0.7', \n",
    "#             '--chunks_to_process', 'all']\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Generate QA pairs using specified API, temperature, and process option.')\n",
    "# parser.add_argument('--context', type=str, required=True, help='One sentence describing the context for the dataset.')\n",
    "# parser.add_argument('--train_output_filename', type=str, required=True)\n",
    "# parser.add_argument('--test_output_filename', type=str, required=True)\n",
    "# parser.add_argument('--api', type=str, choices=['openai', 'runpod'], required=True, help='API to use (openai or runpod)')\n",
    "# parser.add_argument('--temperature', type=float, default=0.1, help='Temperature for generation (between 0 and 1.2)')\n",
    "# parser.add_argument('--chunks_to_process', type=str, choices=['one', 'all'], required=True, help='Process one chunk or all chunks')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Restore the original `sys.argv`\n",
    "# sys.argv = original_argv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use arguments\n",
    "train_output_filename = 'train_output.txt'\n",
    "test_output_filename = 'test_output.txt'\n",
    "api_choice = 'openai'\n",
    "temperature = 0.01\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Company Regulations and Rules.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_output.txt\n",
      "test_output.txt\n",
      "openai\n",
      "0.01\n",
      "all\n",
      "Spirit Airlines Flight Attendant Company Regulations and Rules.\n"
     ]
    }
   ],
   "source": [
    "print(train_output_filename)\n",
    "print(test_output_filename)\n",
    "print(api_choice)\n",
    "print(temperature)\n",
    "print(chunks_to_process)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 22 questions per 1024-token chunk for QA train dataset generation.\n"
     ]
    }
   ],
   "source": [
    "# context\n",
    "tokens_per_question = 45 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 1024 # for testing, set a smaller chunk size.\n",
    "model_context_length = 2048 #This needs to be at least twice the chunk size, otherwise responses will be truncated.\n",
    "\n",
    "train_sample = \"What is the meaning of NOI in the context of Flight Attendants?\\nNOI stands for Notice of Investigation, which is a formal document issued by the Company to a Flight Attendant when an incident or violation has occurred.\\n\\nWhat happens if a Flight Attendant does not pass the second scheduled Recurrent Training class?\\nIf a Flight Attendant does not pass the second scheduled Recurrent Training class, they will be placed on inactive, unpaid status, scheduled for an NOI and subject to termination.\\n\\nWhat is the difference between Line Pay and Block Pay under the Spirit CBA?\\nLine Pay is calculated based on the actual time spent flying, while Block Pay is calculated based on the scheduled flight time.\\n\\nWhat is the purpose of Scheduled and Actual On Duty Limitations in the Spirit CBA?\\nSection 7.B.1 Scheduled On Duty Limitations outlines that a Flight Attendant will not be scheduled or re-scheduled to remain on duty for more than fourteen (14) hours, with certain exceptions mentioned in the text. Section 7.B.2 Actual On Duty Limitations outlines that a Flight attendant may remain on duty up to 16 hours due to delays or other events. These can be waived by asking to extend or being asked to extend under a case by case basis.\"\n",
    "test_sample = \"What is the meaning of NOI in the context of Flight Attendants?\\nNOI stands for Notice of Investigation, which is a formal document issued by the Company to a Flight Attendant when an incident or violation has occurred.\\n\\nWhat happens if a Flight Attendant does not pass the second scheduled Recurrent Training class?\\nIf a Flight Attendant does not pass the second scheduled Recurrent Training class, they will be placed on inactive, unpaid status, scheduled for an NOI and subject to termination.\\n\\nWhat is the difference between Line Pay and Block Pay under the Spirit CBA?\\nLine Pay is calculated based on the actual time spent flying, while Block Pay is calculated based on the scheduled flight time.\\n\\nWhat is the purpose of Scheduled and Actual On Duty Limitations in the Spirit CBA?\\nSection 7.B.1 Scheduled On Duty Limitations outlines that a Flight Attendant will not be scheduled or re-scheduled to remain on duty for more than fourteen (14) hours, with certain exceptions mentioned in the text. Section 7.B.2 Actual On Duty Limitations outlines that a Flight attendant may remain on duty up to 16 hours due to delays or other events. These can be waived by asking to extend or being asked to extend under a case by case basis.\"\n",
    "\n",
    "questions_per_chunk_train = int(chunk_size / tokens_per_question)\n",
    "questions_per_chunk_test = max(int(questions_per_chunk_train / 10),1)\n",
    "\n",
    "print(f'Setting {questions_per_chunk_train} questions per {int(chunk_size)}-token chunk for QA train dataset generation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of Scheduled and Actual On Duty Limitations in the Spirit CBA?\\nSection 7.B.1 Scheduled On Duty Limitations outlines that a Flight Attendant will not be scheduled or re-scheduled to remain on duty for more than fourteen (14) hours, with certain exceptions mentioned in the text. Section 7.B.2 Actual On Duty Limitations outlines that a Flight attendant may remain on duty up to 16 hours due to delays or other events. These can be waived by asking to extend or being asked to extend under a case by case basis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = sum(1 for _ in encoding.encode(text))\n",
    "    return token_count\n",
    "\n",
    "def read_and_chunk_txt(file_path):\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text = line.strip()\n",
    "            if count_tokens(chunk + text) > chunk_size:\n",
    "                chunks.append(chunk.strip())\n",
    "                chunk = text\n",
    "            else:\n",
    "                chunk += \" \" + text\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in all chunks: 58885\n",
      "Estimated cost with gpt-4: $5.87\n",
      "Estimated cost with gpt-3.5-turbo-16k: $0.13\n"
     ]
    }
   ],
   "source": [
    "chunks = read_and_chunk_txt(\"data/raw_train.txt\")\n",
    "\n",
    "total_tokens = sum(count_tokens(chunk) for chunk in chunks)\n",
    "print(f\"Total tokens in all chunks: {total_tokens}\")\n",
    "\n",
    "estimated_input_tokens = total_tokens * 1.1\n",
    "estimated_output_tokens = total_tokens * 50/tokens_per_question\n",
    "total_estimated_tokens = estimated_input_tokens + estimated_output_tokens\n",
    "\n",
    "estimated_cost_gpt4 = (estimated_input_tokens / 1000 * 0.03) + (estimated_output_tokens  / 1000 * 0.06)\n",
    "estimated_cost_gpt35turbo = (estimated_input_tokens / 1000 * 0.0005) + (estimated_output_tokens  / 1000 * 0.0015)\n",
    "print(f\"Estimated cost with gpt-4: ${estimated_cost_gpt4:.2f}\")\n",
    "print(f\"Estimated cost with gpt-3.5-turbo-16k: ${estimated_cost_gpt35turbo:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# while True:\n",
    "#     if chunks_to_process in ['one', 'all']:\n",
    "#         break\n",
    "#     else:\n",
    "#         print(\"Invalid option. Please enter 'one' or 'all'.\")\n",
    "\n",
    "# snippets = [\n",
    "#     f\"Provide {questions_per_chunk_train} question and answer pair(s) based on the text above. The question should include sufficient information for the answer, without the user having any further context. The answers need not necessarily borrow verbatim from the input text, but they should maintain the meaning. Vary the style and format of questions. Include some tricky and nuanced questions. In certain answers, reverse the order of words compared to how they appear in the input text. Respond in plain text on a new line for each question and answer. Do not include question numbers. Here is an example of a question answer pair:\\n\\n<example>\\n\\n{train_sample}\\n\\n</example>\\n\\n\",\n",
    "#     f\"Provide {questions_per_chunk_test} question and answer pair(s) based on the text above. The question should include sufficient information for the answer, without the user having any further context. The answers should NOT borrow verbatim from the text above, but they should maintain the meaning. Vary the style and format of questions. Respond in plain text on a new line for each question and answer. Do not include question numbers. Here is an example of a question answer pair:\\n\\n<example>\\n\\n{test_sample}\\n\\n</example>\\n\\n\"\n",
    "# ]\n",
    "\n",
    "while True:\n",
    "    if chunks_to_process in ['one', 'all']:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid option. Please enter 'one' or 'all'.\")\n",
    "\n",
    "snippets = [\n",
    "    f\"Provide ample question and answer pair(s) based on the text above to fully cover each aspect. The question should include sufficient information for the answer, without the user having any further context. The answers need not necessarily borrow verbatim from the input text, but they should maintain the meaning. Vary the style and format of questions, utilizing first person and third person. Include some less obvious and nuanced questions. In certain answers, reverse the order of words compared to how they appear in the input text. Respond in plain text on a new line for each question and answer. Do NOT include question numbers. Here is an example of a question answer pair:\\n\\n<example>\\n\\n{train_sample}\\n\\n</example>\\n\\n\",\n",
    "    f\"Provide ample question and answer pair(s) based on the text above to fully cover each aspect. The question should include sufficient information for the answer, without the user having any further context. The answers should NOT borrow verbatim from the text above, but they should maintain the meaning. Vary the style and format of questions. Respond in plain text on a new line for each question and answer. Do not include question numbers. Here is an example of a question answer pair:\\n\\n<example>\\n\\n{test_sample}\\n\\n</example>\\n\\n\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to validate the response format\n",
    "# def is_valid_qa_format(text):\n",
    "#     # Split the text into lines\n",
    "#     lines = text.strip().split('\\n')\n",
    "    \n",
    "#     # Check for at least one question and one answer\n",
    "#     has_question = any(line.endswith('?') for line in lines)\n",
    "#     # has_answer = len(lines) > 1 and any(not line.endswith('?') for line in lines)  # Assuming answer lines don't end with a question mark\n",
    "    \n",
    "#     # Basic syntax safety check (example: avoid null bytes which can be problematic)\n",
    "#     syntax_safe = all('\\0' not in line for line in lines)\n",
    "    \n",
    "#     return has_question and syntax_safe\n",
    "\n",
    "# def clean_text(text):\n",
    "#     # Remove control characters except newline\n",
    "#     text = re.sub(r'[\\x00-\\x09\\x0b-\\x1F\\x7F]', '', text)\n",
    "    \n",
    "#     # Escape potentially dangerous characters or sequences\n",
    "#     # This is an example; adapt based on your context\n",
    "#     text = re.sub(r'[<>{};`]', '', text)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# # Function to log errors\n",
    "# def log_error(prompt, response, filename=\"error_log.txt\"):\n",
    "#     with open(filename, \"a\", encoding='utf-8') as error_file:\n",
    "#         error_file.write(f\"Prompt: {prompt}\\nResponse: {response}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate the response format\n",
    "def is_valid_qa_format(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "    \n",
    "    # Check for at least one question and one answer\n",
    "    has_question = any(line.endswith('?') for line in lines)\n",
    "    # has_answer = len(lines) > 1 and any(not line.endswith('?') for line in lines)  # Assuming answer lines don't end with a question mark\n",
    "    \n",
    "    # Basic syntax safety check (example: avoid null bytes which can be problematic)\n",
    "    syntax_safe = all('\\0' not in line for line in lines)\n",
    "    \n",
    "    return has_question and syntax_safe\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove control characters except newline\n",
    "    text = re.sub(r'[\\x00-\\x09\\x0b-\\x1F\\x7F]', '', text)\n",
    "    \n",
    "    # Escape potentially dangerous characters or sequences\n",
    "    # This is an example; adapt based on your context\n",
    "    text = re.sub(r'[<>{};`]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to log errors\n",
    "def log_error(prompt, response, filename=\"error_log.txt\"):\n",
    "    with open(filename, \"a\", encoding='utf-8') as error_file:\n",
    "        error_file.write(f\"Prompt: {prompt}\\nResponse: {response}\\n\\n\")\n",
    "\n",
    "# Update your loop for processing chunks\n",
    "for idx, snippet in enumerate(snippets):\n",
    "    # print(snippet)\n",
    "    output_filename = f\"data/{train_output_filename}\" if idx == 0 else f\"data/{test_output_filename}\"\n",
    "    \n",
    "    with open(output_filename, \"a\", encoding='utf-8') as output_file:\n",
    "        if api_choice == \"openai\":\n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                prompt = f\"<input-text>\\n\\nContext: {context}\\n\\nText:\\n\\n{chunk}\\n\\n</input-text>\\n\\n{snippet}\"\n",
    "                if chunks_to_process == 'one' and chunk_idx > 0:\n",
    "                    break\n",
    "                if chunks_to_process == 'one':\n",
    "                    print(f\"The prompt is:\\n\\n{prompt}\")\n",
    "\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=\"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\",\n",
    "                    temperature=temperature,\n",
    "                    messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                )\n",
    "                max_tokens=int(model_context_length * 0.9) # Ensure you have defined max_tokens appropriately)\n",
    "\n",
    "                response = completion.choices[0].message.content\n",
    "                # print(f\"\\n\\nRaw Response:\\n\\n{response}\")\n",
    "                \n",
    "                cleaned_response = clean_text(response)\n",
    "\n",
    "                # print(f\"\\n\\nCleaned Response:\\n\\n{cleaned_response}\")\n",
    "\n",
    "                if is_valid_qa_format(cleaned_response):\n",
    "                    output_file.write(cleaned_response + \"\\n\\n\")\n",
    "                else:\n",
    "                    log_error(prompt, cleaned_response, filename=f\"error_log_{idx}.txt\") # Log the error with a unique file per snippet\n",
    "\n",
    "                output_file.flush()\n",
    "                time.sleep(0.2)\n",
    "\n",
    "        elif api_choice == \"runpod\":\n",
    "            max_tokens = int(model_context_length * 0.9)\n",
    "            if chunks_to_process == 'all':\n",
    "                with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                    prompt = f\"<input-text>\\n\\nContext: {context}\\n\\nText:\\n\\n{chunk}\\n\\n</input-text>\\n\\n{snippet}\"\n",
    "                    future_to_chunk = {executor.submit(query_runpod, pod_id, prompt, max_tokens, temperature): chunk for chunk in chunks}\n",
    "                    for future in concurrent.futures.as_completed(future_to_chunk):\n",
    "                        chunk = future_to_chunk[future]\n",
    "                        try:\n",
    "                            response = future.result()\n",
    "                        except Exception as exc:\n",
    "                            print(f\"Generated an exception: {exc}\")\n",
    "                        else:\n",
    "                            if is_valid_qa_format(response):\n",
    "                                output_file.write(response + \"\\n\\n\")\n",
    "                            else:\n",
    "                                log_error(chunk, response, filename=f\"error_log_{idx}.txt\") # Log the error with a unique file per snippet\n",
    "\n",
    "                            output_file.flush()\n",
    "            else:\n",
    "                for chunk in chunks:\n",
    "                    prompt = f\"<input-text>\\n\\nContext: {context}\\n\\nText:\\n\\n{chunk}\\n\\n</input-text>\\n\\n{snippet}\"\n",
    "                    # print(f\"Prompt:\\n\\n{prompt}\")\n",
    "                    response = query_runpod(pod_id, prompt, max_tokens, temperature)\n",
    "                    # print(f\"\\n\\nResponse:\\n\\n{response}\")\n",
    "                    cleaned_response = clean_text(response)\n",
    "                    # print(f\"Cleaned Response:\\n\\n{cleaned_response}\")\n",
    "                    if is_valid_qa_format(cleaned_response):\n",
    "                        output_file.write(cleaned_response + \"\\n\\n\")\n",
    "                    else:\n",
    "                        log_error(chunk, cleaned_response, filename=f\"error_log_{idx}.txt\")\n",
    "\n",
    "                    output_file.flush()\n",
    "\n",
    "                    if chunks_to_process == 'one':\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above needs to be run in .venv to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDIT ME TO COPY WITH FORMATTING\n",
    "\n",
    "<!-- Run 1:\n",
    "api_choice = 'openai'\n",
    "temperature = 0.01\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Collective Bargaining Agreement and Rules.'\n",
    "\n",
    "# context\n",
    "tokens_per_question = 45 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 2000 # for testing, set a smaller chunk size.\n",
    "model_context_length = 4000 #This needs to be at least twice the chunk size, otherwise responses will be truncated. -->\n",
    "\n",
    "<!-- Run 2:\n",
    "api_choice = 'openai'\n",
    "temperature = 0.05\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Collective Bargaining Agreement and Rules.'\n",
    "\n",
    "# context\n",
    "tokens_per_question = 45 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 2000 # for testing, set a smaller chunk size.\n",
    "model_context_length = 4000 #This needs to be at least twice the chunk size, otherwise responses will be truncated. -->\n",
    "\n",
    "<!-- Run 3:\n",
    "api_choice = 'openai'\n",
    "temperature = 0.1\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Contractual Regulations and Rules.'\n",
    "\n",
    "# context\n",
    "tokens_per_question = 45 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 2000 # for testing, set a smaller chunk size.\n",
    "model_context_length = 4000 #This needs to be at least twice the chunk size, otherwise responses will be truncated. -->\n",
    "\n",
    "<!-- Run 4:\n",
    "api_choice = 'openai'\n",
    "temperature = 0.15\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Contractual Regulations and Flight Attendant Rules.'\n",
    "\n",
    "# context\n",
    "tokens_per_question = 55 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 2000 # for testing, set a smaller chunk size.\n",
    "model_context_length = 4000 #This needs to be at least twice the chunk size, otherwise responses will be truncated. -->\n",
    "\n",
    "<!-- Run 5:\n",
    "api_choice = 'openai'\n",
    "temperature = 0.2\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Company Regulations and Flight Attendant Rules.'\n",
    "\n",
    "# context\n",
    "tokens_per_question = 75 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 2000 # for testing, set a smaller chunk size.\n",
    "model_context_length = 4000 #This needs to be at least twice the chunk size, otherwise responses will be truncated. -->\n",
    "\n",
    "<!-- Run 6:\n",
    "api_choice = 'openai'\n",
    "temperature = 0.25\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Company Regulations and Rules.'\n",
    "\n",
    "# context\n",
    "tokens_per_question = 100 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 2000 # for testing, set a smaller chunk size.\n",
    "model_context_length = 4000 #This needs to be at least twice the chunk size, otherwise responses will be truncated. -->\n",
    "\n",
    "<!-- Run 7:\n",
    "api_choice = 'openai'\n",
    "temperature = 0.3\n",
    "chunks_to_process = 'all'\n",
    "context = 'Spirit Airlines Flight Attendant Company Regulations and Rules.'\n",
    "\n",
    "# context\n",
    "tokens_per_question = 125 # reduce this parameter to increase the granularity of questions. If you reduce this too much the language model may hallucinate content.\n",
    "# chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)/2 # there are empirically about 60 tokens per QA pair. Also, GPT gets confused making too many questions up.\n",
    "chunk_size = 2000 # for testing, set a smaller chunk size.\n",
    "model_context_length = 4000 #This needs to be at least twice the chunk size, otherwise responses will be truncated. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
